
working with sql Queries:
------------------------

For working with sql queries,Register DataFrame as table and apply all valid sql queries on it

to register DF as table--->we have 2 ways

1.df.registerTempTable("tablename")
2.sqlContext.registerDataFrameAsTable(df,"tablename")

sqlContext.sql("........sql query........").show()

>>> r1=sc.textFile("hdfs://localhost:9000/sparklab/emps1")
>>> r1.collect()
[Stage 0:>                                                          (0 + 1) /                                                                               [u'101,aaa,1000,m,11', u'102,bbb,2000,f,12', u'103,ccc,3000,m,12', u'104,ddd,4000,f,13', u'105,eee,5000,m,11', u'106,fff,6000,f,14', u'107,ggg,7000,m,15', u'108,hhh,8000,f,16']
>>> r2=r1.map(lambda x:x.split(","))
>>> r2.collect()
[[u'101', u'aaa', u'1000', u'm', u'11'], [u'102', u'bbb', u'2000', u'f', u'12'], [u'103', u'ccc', u'3000', u'm', u'12'], [u'104', u'ddd', u'4000', u'f', u'13'], [u'105', u'eee', u'5000', u'm', u'11'], [u'106', u'fff', u'6000', u'f', u'14'], [u'107', u'ggg', u'7000', u'm', u'15'], [u'108', u'hhh', u'8000', u'f', u'16']]

>>> from pyspark.sql import Row
>>> r3=r2.map(lambda x:Row(eid=int(x[0]),ename=x[1],sal=int(x[2]),sex=x[3],dno=int(x[4])))
>>> r3.collect()
[Row(dno=11, eid=101, ename=u'aaa', sal=1000, sex=u'm'), Row(dno=12, eid=102, ename=u'bbb', sal=2000, sex=u'f'), Row(dno=12, eid=103, ename=u'ccc', sal=3000, sex=u'm'), Row(dno=13, eid=104, ename=u'ddd', sal=4000, sex=u'f'), Row(dno=11, eid=105, ename=u'eee', sal=5000, sex=u'm'), Row(dno=14, eid=106, ename=u'fff', sal=6000, sex=u'f'), Row(dno=15, eid=107, ename=u'ggg', sal=7000, sex=u'm'), Row(dno=16, eid=108, ename=u'hhh', sal=8000, sex=u'f')]

>>> emps=spark.createDataFrame(r3)
>>> emps.show()
+---+---+-----+----+---+
|dno|eid|ename| sal|sex|
+---+---+-----+----+---+
| 11|101|  aaa|1000|  m|
| 12|102|  bbb|2000|  f|
| 12|103|  ccc|3000|  m|
| 13|104|  ddd|4000|  f|
| 11|105|  eee|5000|  m|
| 14|106|  fff|6000|  f|
| 15|107|  ggg|7000|  m|
| 16|108|  hhh|8000|  f|
+---+---+-----+----+---+
I-way:
-----
>>>emps.registerTempTable("emps1")
>>> e1=sqlContext.sql("select * from emps1")
>>> e1.show()
+---+---+-----+----+---+
|dno|eid|ename| sal|sex|
+---+---+-----+----+---+
| 11|101|  aaa|1000|  m|
| 12|102|  bbb|2000|  f|
| 12|103|  ccc|3000|  m|
| 13|104|  ddd|4000|  f|
| 11|105|  eee|5000|  m|
| 14|106|  fff|6000|  f|
| 15|107|  ggg|7000|  m|
| 16|108|  hhh|8000|  f|
+---+---+-----+----+---+
#or
II-way

 sqlContext.registerDataFrameAsTable(emps,"emps2")
>>> e2=sqlContext.sql("select * from emps2")
>>> e2.show()
+---+---+-----+----+---+
|dno|eid|ename| sal|sex|
+---+---+-----+----+---+
| 11|101|  aaa|1000|  m|
| 12|102|  bbb|2000|  f|
| 12|103|  ccc|3000|  m|
| 13|104|  ddd|4000|  f|
| 11|105|  eee|5000|  m|
| 14|106|  fff|6000|  f|
| 15|107|  ggg|7000|  m|
| 16|108|  hhh|8000|  f|
+---+---+-----+----+---+

Here emps2 is the table name ,on this table,we can perform any valid sql queries

res1=sqlContext.sql("select * from emps2").show()
+---+---+-----+----+---+
|dno|eid|ename| sal|sex|
+---+---+-----+----+---+
| 11|101|  aaa|1000|  m|
| 12|102|  bbb|2000|  f|
| 12|103|  ccc|3000|  m|
| 13|104|  ddd|4000|  f|
| 11|105|  eee|5000|  m|
| 14|106|  fff|6000|  f|
| 15|107|  ggg|7000|  m|
| 16|108|  hhh|8000|  f|
+---+---+-----+----+---+

HDFSfile----->RDD----->DF------>table------>sqlquery

-----------------------------------------------------------------------------------------------
adding extra column:

>>> taxadd=sqlContext.sql("select *,sal*0.10 as tax from emps2").show()
+---+---+-----+----+---+------+
|dno|eid|ename| sal|sex|   tax|
+---+---+-----+----+---+------+
| 11|101|  aaa|1000|  m|100.00|
| 12|102|  bbb|2000|  f|200.00|
| 12|103|  ccc|3000|  m|300.00|
| 13|104|  ddd|4000|  f|400.00|
| 11|105|  eee|5000|  m|500.00|
| 14|106|  fff|6000|  f|600.00|
| 15|107|  ggg|7000|  m|700.00|
| 16|108|  hhh|8000|  f|800.00|
+---+---+-----+----+---+------+

----------------------------------------------------------------------------------------------
#case 1: Single grouping and Single Aggregation

>>> singlegrp_aggr=sqlContext.sql("select sex,sum(sal) from emps2 group by sex").show()

|sex|sum(sal)|
+---+--------+
|  m|   16000|
|  f|   20000|
+---+--------+


--------------------------------------------------------------------------------------------
#case 2: multigrouping single Aggregation:

dnowise,sexwise---->sum(sal)

>>> multigrp=sqlContext.sql("select dno,sex,sum(sal) from emps2 group by dno,sex").show()

+---+---+--------+
|dno|sex|sum(sal)|
+---+---+--------+
| 12|  f|    2000|
| 12|  m|    3000|
| 16|  f|    8000|
| 13|  f|    4000|
| 11|  m|    6000|
| 15|  m|    7000|
| 14|  f|    6000|
+---+---+--------+

---------------------------------------------------------------------------------------------
#case3: single grouping and multiple Aggregation:
        ----------------------------------------

>>> mulaggr=sqlContext.sql("select sex,sum(sal),avg(sal),max(sal),min(sal),count(*) from emps2 
group by sex").show()

+---+--------+--------+--------+--------+--------+
|sex|sum(sal)|avg(sal)|max(sal)|min(sal)|count(1)|
+---+--------+--------+--------+--------+--------+
|  m|   16000|  4000.0|    7000|    1000|       4|
|  f|   20000|  5000.0|    8000|    2000|       4|
+---+--------+--------+--------+--------+--------+

#---------------------------------------------------------------------------------------------
#Case 4: multi grouping and multiple Aggregation:

>>> mulgrp_aggr=sqlContext.sql("select dno,sex,sum(sal),avg(sal),max(sal),min(sal),count(*) from emps2 group by dno,sex").show()

+---+---+--------+--------+--------+--------+--------+
|dno|sex|sum(sal)|avg(sal)|max(sal)|min(sal)|count(1)|
+---+---+--------+--------+--------+--------+--------+
| 12|  f|    2000|  2000.0|    2000|    2000|       1|
| 12|  m|    3000|  3000.0|    3000|    3000|       1|
| 16|  f|    8000|  8000.0|    8000|    8000|       1|
| 13|  f|    4000|  4000.0|    4000|    4000|       1|
| 11|  m|    6000|  3000.0|    5000|    1000|       2|
| 15|  m|    7000|  7000.0|    7000|    7000|       1|
| 14|  f|    6000|  6000.0|    6000|    6000|       1|
+---+---+--------+--------+--------+--------+--------+

#----------------------------------------------------------------------------------------------                
Joins:
------
Task: For each city,I want the total sal budget

sal----->emp table
city---->dept table
              
hadoop@ubuntu:~$ hdfs dfs -cat /sparklab/emps1
101,aaa,1000,m,11
102,bbb,2000,f,12
103,ccc,3000,m,12
104,ddd,4000,f,13
105,eee,5000,m,11
106,fff,6000,f,14
107,ggg,7000,m,15
108,hhh,8000,f,16

hadoop@ubuntu:~$ hdfs dfs -cat /sparklab/dept1
11,mrkt,hyd
12,HR,delhi
13,fin,pune
17,HR,hyd
18,fin,pune
19,mrkt,delhi

step1: Loading both emp and dept from HDFS

>>> r1=sc.textFile("hdfs://localhost:9000/sparklab/emps1")
>>> r1.collect()
[u'101,aaa,1000,m,11', u'102,bbb,2000,f,12', u'103,ccc,3000,m,12', u'104,ddd,4000,f,13', u'105,eee,5000,m,11', u'106,fff,6000,f,14', u'107,ggg,7000,m,15', u'108,hhh,8000,f,16']

>>> r2=r1.map(lambda x:x.split(","))
>>> r2.collect()
[[u'101', u'aaa', u'1000', u'm', u'11'], [u'102', u'bbb', u'2000', u'f', u'12'], [u'103', u'ccc', u'3000', u'm', u'12'], [u'104', u'ddd', u'4000', u'f', u'13'], [u'105', u'eee', u'5000', u'm', u'11'], [u'106', u'fff', u'6000', u'f', u'14'], [u'107', u'ggg', u'7000', u'm', u'15'], [u'108', u'hhh', u'8000', u'f', u'16']]

>>> from pyspark.sql import Row
>>> r3=r2.map(lambda x:Row(eid=int(x[0]),ename=x[1],sal=int(x[2]),sex=x[3],dno=int(x[4])))
>>> r3.collect()
[Row(dno=11, eid=101, ename=u'aaa', sal=1000, sex=u'm'), Row(dno=12, eid=102, ename=u'bbb', sal=2000, sex=u'f'), Row(dno=12, eid=103, ename=u'ccc', sal=3000, sex=u'm'), Row(dno=13, eid=104, ename=u'ddd', sal=4000, sex=u'f'), Row(dno=11, eid=105, ename=u'eee', sal=5000, sex=u'm'), Row(dno=14, eid=106, ename=u'fff', sal=6000, sex=u'f'), Row(dno=15, eid=107, ename=u'ggg', sal=7000, sex=u'm'), Row(dno=16, eid=108, ename=u'hhh', sal=8000, sex=u'f')]

>>> emps1=spark.createDataFrame(r3)

>>> emps1.show()
+---+---+-----+----+---+
|dno|eid|ename| sal|sex|
+---+---+-----+----+---+
| 11|101|  aaa|1000|  m|
| 12|102|  bbb|2000|  f|
| 12|103|  ccc|3000|  m|
| 13|104|  ddd|4000|  f|
| 11|105|  eee|5000|  m|
| 14|106|  fff|6000|  f|
| 15|107|  ggg|7000|  m|
| 16|108|  hhh|8000|  f|
+---+---+-----+----+---+

--------------------------------------------
Now loading dept file from HDFS

>>> rr1=sc.textFile("hdfs://localhost:9000/sparklab/dept1")
>>> rr1.collect()
[u'11,mrkt,hyd', u'12,HR,delhi', u'13,fin,pune', u'17,HR,hyd', u'18,fin,pune', u'19,mrkt,delhi']
>>> rr2=rr1.map(lambda x:x.split(","))
>>> rr2.collect()
[[u'11', u'mrkt', u'hyd'], [u'12', u'HR', u'delhi'], [u'13', u'fin', u'pune'], [u'17', u'HR', u'hyd'], [u'18', u'fin', u'pune'], [u'19', u'mrkt', u'delhi']]
>>> from pyspark.sql import Row
>>> rr3=rr2.map(lambda x:Row(dno=int(x[0]),dname=x[1],city=x[2]))
>>> rr3.collect()
[Row(city=u'hyd', dname=u'mrkt', dno=11), Row(city=u'delhi', dname=u'HR', dno=12), Row(city=u'pune', dname=u'fin', dno=13), Row(city=u'hyd', dname=u'HR', dno=17), Row(city=u'pune', dname=u'fin', dno=18), Row(city=u'delhi', dname=u'mrkt', dno=19)]
>>> dept=spark.createDataFrame(rr3)
>>> dept.show()
+-----+-----+---+
| city|dname|dno|
+-----+-----+---+
|  hyd| mrkt| 11|
|delhi|   HR| 12|
| pune|  fin| 13|
|  hyd|   HR| 17|
| pune|  fin| 18|
|delhi| mrkt| 19|
+-----+-----+---+

Now registering DF as table

>>> 
>>> sqlContext.registerDataFrameAsTable(emps1,"emps2")
>>> sqlContext.registerDataFrameAsTable(dept,"dept1")
>>> sqlContext.sql("select * from emps2").show()
+---+---+-----+----+---+
|dno|eid|ename| sal|sex|
+---+---+-----+----+---+
| 11|101|  aaa|1000|  m|
| 12|102|  bbb|2000|  f|
| 12|103|  ccc|3000|  m|
| 13|104|  ddd|4000|  f|
| 11|105|  eee|5000|  m|
| 14|106|  fff|6000|  f|
| 15|107|  ggg|7000|  m|
| 16|108|  hhh|8000|  f|
+---+---+-----+----+---+

>>> sqlContext.sql("select * from dept1").show()
+-----+-----+---+
| city|dname|dno|
+-----+-----+---+
|  hyd| mrkt| 11|
|delhi|   HR| 12|
| pune|  fin| 13|
|  hyd|   HR| 17|
| pune|  fin| 18|
|delhi| mrkt| 19|
+-----+-----+---+

>>> res=sqlContext.sql("select city,sum(sal) as totalsal from emps2 e join dept1 d on e.dno=d.dno group by city").show()

+-----+--------+
| city|totalsal|
+-----+--------+
|delhi|    5000|
|  hyd|    6000|
| pune|    4000|
+-----+--------+
-----------------------------------------------------------------------------------------------
#Task:
hadoop@ubuntu:~$ nano sales5.txt
hadoop@ubuntu:~$ cat sales5.txt
1/2/2017,70000
2/2/2017,20000
3/2/2017,30000
4/2/2017,15000
1/3/2017,9000
2/3/2017,11000
3/3/2017,19000
4/3/2017,22000
1/5/2016,15000
2/5/2016,13000
1/7/2015,4000
2/7/2015,6000
1/8/2015,77000
2/8/2015,44000
1/9/2015,30000
2/9/2015,20000
hadoop@ubuntu:~$ hdfs dfs -put sales5.txt /sparklab
hadoop@ubuntu:~$ hdfs dfs -cat /sparklab/sales5.txt
1/2/2017,70000
2/2/2017,20000
3/2/2017,30000
4/2/2017,15000
1/3/2017,9000
2/3/2017,11000
3/3/2017,19000
4/3/2017,22000
1/5/2016,15000
2/5/2016,13000
1/7/2015,4000
2/7/2015,6000
1/8/2015,77000
2/8/2015,44000
1/9/2015,30000
2/9/2015,20000
hadoop@ubuntu:~$ 

Task 1: select year,sum(price),max(price),min(price),count(*) from sales5 group by year
Task 2: select year,month,sum(price),max(price),min(price),count(*) from sales5 group by year,month

>>> #step 1: loading
... 
>>> r1=sc.textFile("hdfs://localhost:9000/sparklab/sales5.txt")
>>> r1.collect()
[u'1/2/2017,70000', u'2/2/2017,20000', u'3/2/2017,30000', u'4/2/2017,15000', u'1/3/2017,9000', u'2/3/2017,11000', u'3/3/2017,19000', u'4/3/2017,22000', u'1/5/2016,15000', u'2/5/2016,13000', u'1/7/2015,4000', u'2/7/2015,6000', u'1/8/2015,77000', u'2/8/2015,44000', u'1/9/2015,30000', u'2/9/2015,20000']

>>> #step 2 :splitting based on comma
... 
>>> r2=r1.map(lambda x:x.split(","))
>>> r2.collect()
[[u'1/2/2017', u'70000'], [u'2/2/2017', u'20000'], [u'3/2/2017', u'30000'], [u'4/2/2017', u'15000'], [u'1/3/2017', u'9000'], [u'2/3/2017', u'11000'], [u'3/3/2017', u'19000'], [u'4/3/2017', u'22000'], [u'1/5/2016', u'15000'], [u'2/5/2016', u'13000'], [u'1/7/2015', u'4000'], [u'2/7/2015', u'6000'], [u'1/8/2015', u'77000'], [u'2/8/2015', u'44000'], [u'1/9/2015', u'30000'], [u'2/9/2015', u'20000']]

>>> #step 3: splitting based on '/'
... 
>>> r3=r2.map(lambda x:(x[0].split("/"),x[1]))
>>> r3.collect()
[([u'1', u'2', u'2017'], u'70000'), ([u'2', u'2', u'2017'], u'20000'), ([u'3', u'2', u'2017'], u'30000'), ([u'4', u'2', u'2017'], u'15000'), ([u'1', u'3', u'2017'], u'9000'), ([u'2', u'3', u'2017'], u'11000'), ([u'3', u'3', u'2017'], u'19000'), ([u'4', u'3', u'2017'], u'22000'), ([u'1', u'5', u'2016'], u'15000'), ([u'2', u'5', u'2016'], u'13000'), ([u'1', u'7', u'2015'], u'4000'), ([u'2', u'7', u'2015'], u'6000'), ([u'1', u'8', u'2015'], u'77000'), ([u'2', u'8', u'2015'], u'44000'), ([u'1', u'9', u'2015'], u'30000'), ([u'2', u'9', u'2015'], u'20000')]


>>> #step 4: Extracting the required fields and creating row objects
... 
>>> from pyspark.sql import Row
>>> r4=r3.map(lambda x:Row(day=int(x[0][0]),month=int(x[0][1]),year=int(x[0][2]),price=int(x[1])))
>>> r4.collect()
[Row(day=1, month=2, price=70000, year=2017), Row(day=2, month=2, price=20000, year=2017), Row(day=3, month=2, price=30000, year=2017), Row(day=4, month=2, price=15000, year=2017), Row(day=1, month=3, price=9000, year=2017), Row(day=2, month=3, price=11000, year=2017), Row(day=3, month=3, price=19000, year=2017), Row(day=4, month=3, price=22000, year=2017), Row(day=1, month=5, price=15000, year=2016), Row(day=2, month=5, price=13000, year=2016), Row(day=1, month=7, price=4000, year=2015), Row(day=2, month=7, price=6000, year=2015), Row(day=1, month=8, price=77000, year=2015), Row(day=2, month=8, price=44000, year=2015), Row(day=1, month=9, price=30000, year=2015), Row(day=2, month=9, price=20000, year=2015)]


step 5: Converting RDD to DF

>>> sales=spark.createDataFrame(r4)
>>> sales.show()
+---+-----+-----+----+
|day|month|price|year|
+---+-----+-----+----+
|  1|    2|70000|2017|
|  2|    2|20000|2017|
|  3|    2|30000|2017|
|  4|    2|15000|2017|
|  1|    3| 9000|2017|
|  2|    3|11000|2017|
|  3|    3|19000|2017|
|  4|    3|22000|2017|
|  1|    5|15000|2016|
|  2|    5|13000|2016|
|  1|    7| 4000|2015|
|  2|    7| 6000|2015|
|  1|    8|77000|2015|
|  2|    8|44000|2015|
|  1|    9|30000|2015|
|  2|    9|20000|2015|
+---+-----+-----+----+

step 6: register DF as Table

>>> sqlContext.registerDataFrameAsTable(sales,"sales1")
>>> sqlContext.sql("select * from sales1").show()
+---+-----+-----+----+
|day|month|price|year|
+---+-----+-----+----+
|  1|    2|70000|2017|
|  2|    2|20000|2017|
|  3|    2|30000|2017|
|  4|    2|15000|2017|
|  1|    3| 9000|2017|
|  2|    3|11000|2017|
|  3|    3|19000|2017|
|  4|    3|22000|2017|
|  1|    5|15000|2016|
|  2|    5|13000|2016|
|  1|    7| 4000|2015|
|  2|    7| 6000|2015|
|  1|    8|77000|2015|
|  2|    8|44000|2015|
|  1|    9|30000|2015|
|  2|    9|20000|2015|
+---+-----+-----+----+

step 7:

>>> res1=sqlContext.sql("select year,sum(price) as totalrevenue from sales1 group by year").show()
+----+------------+
|year|totalrevenue|
+----+------------+
|2016|       28000|
|2017|      196000|
|2015|      181000|
+----+------------+

>>> res1=sqlContext.sql("select year,month,sum(price) as totalrevenue from sales1 group by year,month").show()
+----+-----+------------+
|year|month|totalrevenue|
+----+-----+------------+
|2015|    7|       10000|
|2015|    8|      121000|
|2017|    2|      135000|
|2016|    5|       28000|
|2017|    3|       61000|
|2015|    9|       50000|
+----+-----+------------+


-----------------------------------------------------------------------------------------------





























































