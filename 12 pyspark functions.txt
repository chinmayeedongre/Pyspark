Pyspark functions:

1)withColumn():
  we can perform the following
  i)for changing column datatype
 ii)modifying or update column value
iii)Deriving new column from the existing column
iv)Renaming a column
v)dropping dataframe column
  

>>> data=[(101,'Miller',50000,'2020-02-21','M','pune'),
       (102,'Blake',60000,'2019-04-15','M','hyd'),
       (103,'Priya',30000,'2021-05-22','F','pune'),
       (104,'Sony',70000,'2018-05-22','F','hyd')]

>>> columns=["Empid","Empname","SAlary","JoinDate","Sex","City"]

>>> df=spark.createDataFrame(data,columns)
>>>df.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

i)for changing column datatype
 -for changing the datatype, we use cast() function along with withColumn()

>>> df.printSchema()
root
 |-- Empid: long (nullable = true)
 |-- Empname: string (nullable = true)
 |-- SAlary: long (nullable = true)
 |-- JoinDate: string (nullable = true)
 |-- Sex: string (nullable = true)
 |-- City: string (nullable = true)

>>> from pyspark.sql.functions import col
>>> df1=df.withColumn("SAlary",col("SAlary").cast("Integer"))
>>> df1.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

>>> df1.printSchema()
root
 |-- Empid: long (nullable = true)
 |-- Empname: string (nullable = true)
 |-- SAlary: integer (nullable = true)
 |-- JoinDate: string (nullable = true)
 |-- Sex: string (nullable = true)
 |-- City: string (nullable = true)
---------------------------------------------------------------------------------
ii)modifying or updating column value
   i)updating with a value
  ii)updating based on condition

>>> #incrementing the salaries with 20% hike
... 
>>> df2=df.withColumn("SAlary",df.SAlary+df.SAlary*0.20)
>>> df2.show()
+-----+-------+-------+----------+---+----+
|Empid|Empname| SAlary|  JoinDate|Sex|City|
+-----+-------+-------+----------+---+----+
|  101| Miller|60000.0|2020-02-21|  M|pune|
|  102|  Blake|72000.0|2019-04-15|  M| hyd|
|  103|  Priya|36000.0|2021-05-22|  F|pune|
|  104|   Sony|84000.0|2018-05-22|  F| hyd|
+-----+-------+-------+----------+---+----+

>>> #adding 5000 as bonus to each employee
... 
>>> df3=df.withColumn("SAlary",df.SAlary+5000)
>>> df3.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 55000|2020-02-21|  M|pune|
|  102|  Blake| 65000|2019-04-15|  M| hyd|
|  103|  Priya| 35000|2021-05-22|  F|pune|
|  104|   Sony| 75000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

ii)updating based on condition:
   -here we use withColumn() along with when
     modify---->"M" ------>"MALE"
                "F"------->"FEMALE"

>>> from pyspark.sql.functions import when
>>> df4=df.withColumn("Sex",when(df.Sex == "M","MALE") \
...       .when(df.Sex == 'F',"FEMALE") \
...       .otherwise(df.Sex))
>>> df4.show()
+-----+-------+------+----------+------+----+
|Empid|Empname|SAlary|  JoinDate|   Sex|City|
+-----+-------+------+----------+------+----+
|  101| Miller| 50000|2020-02-21|  MALE|pune|
|  102|  Blake| 60000|2019-04-15|  MALE| hyd|
|  103|  Priya| 30000|2021-05-22|FEMALE|pune|
|  104|   Sony| 70000|2018-05-22|FEMALE| hyd|
+-----+-------+------+----------+------+----+

-----------------------------------------------------------------------------------------------
3) Adding new column:
   -Adding new column with default/constant/None/Null value 
   -Adding new column based on another column
   -Adding column based on condition

i)Adding new column with constant
   -here we use lit() function 

>>> #add a new column hike_percent
... 
>>> from pyspark.sql.functions import lit
>>> df5=df.withColumn("Hike_percent",lit(0.30))
>>> df5.show()
+-----+-------+------+----------+---+----+------------+
|Empid|Empname|SAlary|  JoinDate|Sex|City|Hike_percent|
+-----+-------+------+----------+---+----+------------+
|  101| Miller| 50000|2020-02-21|  M|pune|         0.3|
|  102|  Blake| 60000|2019-04-15|  M| hyd|         0.3|
|  103|  Priya| 30000|2021-05-22|  F|pune|         0.3|
|  104|   Sony| 70000|2018-05-22|  F| hyd|         0.3|
+-----+-------+------+----------+---+----+------------+

#Note: If you want to add a NULL/None---->then use lit(None)

ii)Adding column based on another column of a dataframe
   Generate 2 columns---->Tax and netsal based on existing column SAlary

>>> df5=df.withColumn("Tax",df.SAlary*0.10)
>>> df5=df5.withColumn("NetSalary",df5.SAlary-df5.Tax)
>>> df5.show()
+-----+-------+------+----------+---+----+------+---------+
|Empid|Empname|SAlary|  JoinDate|Sex|City|   Tax|NetSalary|
+-----+-------+------+----------+---+----+------+---------+
|  101| Miller| 50000|2020-02-21|  M|pune|5000.0|  45000.0|
|  102|  Blake| 60000|2019-04-15|  M| hyd|6000.0|  54000.0|
|  103|  Priya| 30000|2021-05-22|  F|pune|3000.0|  27000.0|
|  104|   Sony| 70000|2018-05-22|  F| hyd|7000.0|  63000.0|
+-----+-------+------+----------+---+----+------+---------+

iii)Adding column by concatenating existing columns with seperator

>>> from pyspark.sql.functions import concat_ws
>>> df6=df.withColumn("Empid Empname",concat_ws(" ","Empid","Empname"))
>>> df6.show()
+-----+-------+------+----------+---+----+----------+
|Empid|Empname|SAlary|  JoinDate|Sex|City|Empid Empname|
+-----+-------+------+----------+---+----+----------+
|  101| Miller| 50000|2020-02-21|  M|pune|101 Miller|
|  102|  Blake| 60000|2019-04-15|  M| hyd| 102 Blake|
|  103|  Priya| 30000|2021-05-22|  F|pune| 103 Priya|
|  104|   Sony| 70000|2018-05-22|  F| hyd|  104 Sony|
+-----+-------+------+----------+---+----+----------+

iv)Generating new Column based on condition
  
    Ex:  Generate  column Grade
                  if sal>=70000              ----->Grade "A"
                  if sal>=50000 and sal<70000 ---->Grade  "B"
                  else                        ---->Grade  "C"


>>> from pyspark.sql.functions import when,lit
>>> df.withColumn('Grade', \
...   when((df.SAlary >=70000), lit("A"))\
...   .when((df.SAlary < 70000)& (df.SAlary >= 50000), lit("B")) \
...   .otherwise(lit("C"))).show()
+-----+-------+------+----------+---+----+-----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|Grade|
+-----+-------+------+----------+---+----+-----+
|  101| Miller| 50000|2020-02-21|  M|pune|    B|
|  102|  Blake| 60000|2019-04-15|  M| hyd|    B|
|  103|  Priya| 30000|2021-05-22|  F|pune|    C|
|  104|   Sony| 70000|2018-05-22|  F| hyd|    A|
+-----+-------+------+----------+---+----+-----+

----------------------------------------------------------------------------------------------
lit(): lit() function is used to add constant value as a new column
   i)lit() function with select()
>>> from pyspark.sql.functions import col,lit
>>> df2=df.select(col("Empid"),col("Empname"),col("Salary"),lit("IBM").alias("Company"))
>>> df2.show()
+-----+-------+------+-------+
|Empid|Empname|Salary|Company|
+-----+-------+------+-------+
|  101| Miller| 50000|    IBM|
|  102|  Blake| 60000|    IBM|
|  103|  Priya| 30000|    IBM|
|  104|   Sony| 70000|    IBM|
+-----+-------+------+-------+

ii)lit() function with withColumn()

>>> df3=df.withColumn("Loan_Status",when(col("SAlary")>=50000,lit("Eligible")).otherwise(lit("Not Eligible")))
>>> df3.show()
+-----+-------+------+----------+---+----+------------+
|Empid|Empname|SAlary|  JoinDate|Sex|City| Loan_Status|
+-----+-------+------+----------+---+----+------------+
|  101| Miller| 50000|2020-02-21|  M|pune|    Eligible|
|  102|  Blake| 60000|2019-04-15|  M| hyd|    Eligible|
|  103|  Priya| 30000|2021-05-22|  F|pune|Not Eligible|
|  104|   Sony| 70000|2018-05-22|  F| hyd|    Eligible|
+-----+-------+------+----------+---+----+------------+

-----------------------------------------------------------------------------------------------
withColumnRenamed():
  
   using this function we can perform the following
   i)to rename a column
  ii)to rename multiple columns
 iii)Dynamically rename all or multiple columns

syntax:
        withColumnRenamed("oldcolname","newcolname)
   
>>> df7=df.withColumnRenamed("Empid","Ecode")
>>> df7.show()
+-----+-------+------+----------+---+----+
|Ecode|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+


ii)renaming multiple columns
   rename----->SAlary to Income
               Sex    to Gender

>>> df8=df.withColumnRenamed("SAlary","Income") \
...     .withColumnRenamed("Sex","Gender")
>>> df8.show()
+-----+-------+------+----------+------+----+
|Empid|Empname|Income|  JoinDate|Gender|City|
+-----+-------+------+----------+------+----+
|  101| Miller| 50000|2020-02-21|     M|pune|
|  102|  Blake| 60000|2019-04-15|     M| hyd|
|  103|  Priya| 30000|2021-05-22|     F|pune|
|  104|   Sony| 70000|2018-05-22|     F| hyd|
+-----+-------+------+----------+------+----+

iii)to change all the columns in a DF

>>> newcolumns=["col1","col2","col3","col4","col5","col6"]
NameError: name 'newColumns' is not defined
>>> df9=df8.toDF(*newcolumns)
>>> df9.printSchema()
root
 |-- col1: long (nullable = true)
 |-- col2: string (nullable = true)
 |-- col3: long (nullable = true)
 |-- col4: string (nullable = true)
 |-- col5: string (nullable = true)
 |-- col6: string (nullable = true)


----------------------------------------------------------------------------------------------

filter():

We can perform the following
1)Filter with Column Condition

>>> #Filter those whose salaries>50000
... 
>>> df1=df.filter(df.SAlary>50000)
>>> df1.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

>>> #Filter only male emp records
... 
>>> df1=df.filter(df.Sex == 'M')
>>> df1.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
+-----+-------+------+----------+---+----+

>>> #Filter those emps who belongs to other than "Hyd" city
... 
>>> df2=df.filter(df.City != 'hyd')
>>> df2.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  103|  Priya| 30000|2021-05-22|  F|pune|
+-----+-------+------+----------+---+----+

Note: we can use comparision operators for filtering > ,< ,>= ,<= ,== ,!=

2)filtering using col() function

>>> from pyspark.sql.functions import col
>>> # To retrieve a particular row 
... 
>>> df3=df.filter(col("Empid")==104)
>>> df3.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

>>> # filtering specific rows using col()
... 
>>> df4=df.filter(col("City")=="hyd")
>>> df4.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

3)filtering with sql expression

>>> df5=df.filter("Sex == 'M'")  ----here place the entire expression within quotes
>>> df5.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
+-----+-------+------+----------+---+----+

4)filtering with multiple conditions

>>> df6=df.filter((df.SAlary>50000) & (df.City == "hyd"))
>>> df6.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

5)filter based on list values:
  isin(): using this function we can filter the elements which are in list or which are not
          in list

Task: if we have multiple cities ,then filter only those emps who belongs to the cities hyd,pune

>>> list1=["hyd","pune"]
>>> df7=df.filter(df.City.isin(list1))
>>> df7.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

>>> #ex:2
... 
>>> list1=["hyd"]
>>> df8=df.filter(df.City.isin(list1)==False)
>>> df8.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  103|  Priya| 30000|2021-05-22|  F|pune|
+-----+-------+------+----------+---+----+


6)filter based on startswith,endswith
>>> df9=df.filter(df.Empname.startswith("M"))
>>> df9.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
+-----+-------+------+----------+---+----+


>>> df9=df.filter(df.JoinDate.endswith("22"))
>>> df9.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

-----------------------------------------------------------------------------------------------
distinct():

>>> data=[("Ajay","mrkt",30000),
...       ("Miller","sales",40000),
...       ("Blake","Fin",50000),
...       ("Miller","sales",40000),
...       ("Ajay","mrkt",30000)]
>>> columns=["Ename","Dept","Salary"]
>>> df1=spark.createDataFrame(data,columns)
>>> df1.show()
+------+-----+------+
| Ename| Dept|Salary|
+------+-----+------+
|  Ajay| mrkt| 30000|
|Miller|sales| 40000|
| Blake|  Fin| 50000|
|Miller|sales| 40000|
|  Ajay| mrkt| 30000|
+------+-----+------+

>>> resDF=df.distinct()
>>> resDF.show()

|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
|  101| Miller| 50000|2020-02-21|  M|pune|
+-----+-------+------+----------+---+----+

>>> # I want the count of unique employees
... 
>>> print("No. of Employees=",resDF.count())

('No. of Employees=', 4)

>>> print("No. of Employees="+str(resDF.count()))

o/p:
No. of Employees=4
-------------------------------------------------------------
dropDuplicates():

-we can also remove the duplicates using this

>>> df1.show()
+------+-----+------+
| Ename| Dept|Salary|
+------+-----+------+
|  Ajay| mrkt| 30000|
|Miller|sales| 40000|
| Blake|  Fin| 50000|
|Miller|sales| 40000|
|  Ajay| mrkt| 30000|
+------+-----+------+

>>> df2=df1.dropDuplicates()
>>> df2.show()

o/p:
+------+-----+------+
| Ename| Dept|Salary|
+------+-----+------+
|  Ajay| mrkt| 30000|
| Blake|  Fin| 50000|
|Miller|sales| 40000|
+------+-----+------+
--------------------------------------------------------------------------
drop duplicates on selected multiple columns

>>> df3=df1.dropDuplicates(["Dept","Salary"])
>>> df3.show()

o/p:
+------+-----+------+
| Ename| Dept|Salary|
+------+-----+------+
|  Ajay| mrkt| 30000|
| Blake|  Fin| 50000|
|Miller|sales| 40000|
+------+-----+------+

#--------------------------------------------------------------------------------
sort():

>>> df.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

>>> df.sort("SAlary").show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

>>> #sorting based on multiple columns
... 
>>> df.sort("SAlary","Sex").show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

>>> df.sort("city","Sex").show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  104|   Sony| 70000|2018-05-22|  F| hyd|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  101| Miller| 50000|2020-02-21|  M|pune|
+-----+-------+------+----------+---+----+

----------------------------
using col()
>>> from pyspark.sql.functions import col
>>> df.sort(col("city"),col("Sex")).show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  104|   Sony| 70000|2018-05-22|  F| hyd|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  101| Miller| 50000|2020-02-21|  M|pune|
+-----+-------+------+----------+---+----+


----------------------------------------------------
>>> df.sort(df.SAlary.desc()).show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  104|   Sony| 70000|2018-05-22|  F| hyd|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  101| Miller| 50000|2020-02-21|  M|pune|
|  103|  Priya| 30000|2021-05-22|  F|pune|
+-----+-------+------+----------+---+----+

>>> df.sort(df.SAlary.asc()).show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

>>> df.sort(col("SAlary").desc()).show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  104|   Sony| 70000|2018-05-22|  F| hyd|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  101| Miller| 50000|2020-02-21|  M|pune|
|  103|  Priya| 30000|2021-05-22|  F|pune|
+-----+-------+------+----------+---+----+

-------------------------------------------------------------------------
sorting based on Raw sql

>>> df.sort(df.SAlary.desc()).show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  104|   Sony| 70000|2018-05-22|  F| hyd|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  101| Miller| 50000|2020-02-21|  M|pune|
|  103|  Priya| 30000|2021-05-22|  F|pune|
+-----+-------+------+----------+---+----+

>>> df.sort(df.SAlary.asc()).show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

>>> df.sort(col("SAlary").desc()).show()
+-----+-------+------+----------+---+----+
|Empid|Empname|SAlary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  104|   Sony| 70000|2018-05-22|  F| hyd|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  101| Miller| 50000|2020-02-21|  M|pune|
|  103|  Priya| 30000|2021-05-22|  F|pune|
+-----+-------+------+----------+---+----+



































































































































